[TOC]

### 线上nginx耗时问题排查

#### 1.问题现象

最近客户端同学反馈, 服务端getDeviceUserList接口耗时超过了3s, 由于他们设置了3s超时报警，最近一直收到线上偶发性的报警。根据客户端同学反映的request_id(请求唯一标识), 我们发现服务端api-gateway的耗时日志:

```
2020/03/12 16:40:12 [INFO] <middlerwares.go:182> 
__Request__:5cfb9f36-71f1-4885-9a6f-9ae2faec624a 
statusCode:200 clientIP:10.209.146.152 method:POST 
path:/user/getDeviceUserList userAgent:beegoServer ... 
response:{"data":[{"bind_ts":"1573464023","qid":1021551511,"roles":"owner"}],"errmsg":"ok","errno":0,"request_id":"5cfb9f36-71f1-4885-9a6f-9ae2faec624a"} 
timeUsed:1ms
```

日志中显示服务端处理耗时是1ms, 但客户端却说是3s超时？

目前线上服务单机qps约为800左右，机器上同时部署了nginx和api-gateway(基于golang开发)。请求链路如下:

```
client -> lvs -> (nginx->api-gateway)
```

client发起请求到nginx, 反向代理到api-gateway，而目前耗时1ms的日志是api-gateway输出。所以问题可能出现在client和nginx. 

#### 2.问题分析

客户端同学提出是不是由于服务器上nginx的backlog太小，导致客户端请求建立TCP连接时发送的sync包被服务端丢弃:

```
#查看linux上的nginx服务的accept队列
> ss -pl
State Recv-Q Send-Q Local
    ...
LISTEN 0 511 *:https
LISTEN 0 511 *:http
	...
```

显示`Send-Q`为511, `Recv-Q`为0，说明nginx的backlog长度为511(默认值),当前客户端请求连接无堆积。

> **当State为LISTEN状态: Recv-Q 表示当前等待服务端调用 accept 完成三次握手的 listen backlog 数值; Send-Q 表示最大的 listen backlog 数值**

同时我们查询了linux上socket queue的overflowed次数:

```
#任何一个包含dropped或者overflowed并且数值一直居高不下的都不是好现象
> netstat -s
TcpExt:
	...
    52 times the listen queue of a socket overflowed
    52 SYNs to LISTEN sockets ignored
```

并未发现异常，不过我们仍修改了nginx配置做了此处优化:

```
#nginx的server配置,修改backlog为65535
listen 80 backlog=65535;
```

优化之后，客户端同学反馈问题依然存在，由于早期配置nginx时，并没有打印请求耗时，所以我们修改了nginx的配置，增加耗时相关：

```
#nginx的耗时 $request_time $upstream_response_time
http {
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for" $request_time $upstream_response_time';
   ...
}
```

不过由于nginx是业界成熟代理的观念深入人心,主观认为问题应该不在nginx, 所以目光转向client, 是不是客户端自己的问题？

我们与客户端同学沟通并review了其相关源码(基于go1.11并使用beego的http库),我们依据他们的编码方式，针对性的编写了相关代码对服务端接口进行了压测，但仍未得到结论。此时的关注点主要在客户端和网络上,并整理了如下个人疑惑的一些问题:

```
#疑惑问题
1.client出现超时情况时调用服务端接口都是getDeviceUserList? 这个接口请求的正常耗时是多少?
2.client进行ping服务器的耗时是多少？
3.报警机器当前的网络状态是什么情况? netstat -anp | grep TIME | wc -l, 系统有没有进行内核调优
4.报警机器网卡的流量监控? 带宽是多少？
5.客户端出现问题的机器是所有机器？还是某一台,某几台？
```

但客户端同学反映抓到了请求服务端超时的TCP包，就为问题出现在服务端敲定了实锤。

```
#客户端请求服务端的tcp包
#(1) 16:40.09,客户端向服务端发送http请求包
16:40:09.692570 IP (tos 0x0, ttl 64, id 21792, offset 0, flags [DF], proto TCP (6), length 431) cron-iotevent-prod-bac4fe-cbdcc6589-v2h6w.49558 > 10.209.221.234.80: Flags [P.], cksum 0x87c6 (incorrect -> 0x5258), seq 1:392, ack 1, win 229, length 391: HTTP, length: 391
#(2) 16:40.09,服务端ack收到窗口数据
16:40:09.693638 IP (tos 0x0, ttl 56, id 31066, offset 0, flags [DF], proto TCP (6), length 40) 10.209.221.234.80 > cron-iotevent-prod-bac4fe-cbdcc6589-v2h6w.49558: Flags [.], cksum 0x2c8d (correct), ack 392, win 123, length 0	
#(3) 16:40.12,服务端向客户端响应http 
16:40:12.706953 IP (tos 0x0, ttl 56, id 31067, offset 0, flags [DF], proto TCP (6), length 417) 10.209.221.234.80 > cron-iotevent-prod-bac4fe-cbdcc6589-v2h6w.49558: Flags [P.], cksum 0x0fa7 (correct), seq 1:378, ack 393, win 123, length 377: HTTP, length: 377	
```

#### 3.问题解决

再次检查nginx的请求日志，的确发现了超过3s的日志，但是很奇怪api-gateway日志中响应明明是1ms，为什么nginx的日志耗时却是3s? 重新梳理了问题现状:

```
#请求链路
client-->nginx-->api-gateway
#现有依据
1.客户端抓包发现 http请求与响应差3s
2.api-gateway日志显示请求耗时1ms, 并在请求的第3s打印
3.nginx error日志出现批量3s的现象
```

分析得出，当nginx向api-gateway发送http请求时消耗了超过2.99s，原因可能是:

- nginx请求api-gateway的连接出现了拥堵
- nginx内部某些原因的性能消耗

目前nginx配置请求下游时使用的是短连接, 所以将nginx配置修改为长连。同时对线上服务进行压测，使用strace分析nginx进程性能时，发现write部分(日志)占用了23%的耗时:

```
#strace分析进程执行命令,30897为nginx的一个worker进程id
> strace -c -p 30897 #压测30s数据
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 37.41    0.038220           7      5782      3417 accept4
 33.51    0.034240           4      9077           epoll_wait
 23.26    0.023760           5      4730           write
  1.99    0.002031           0      4730           close
  1.45    0.001485           0      4730           writev
  1.04    0.001061           0      2365      2365 connect
  0.46    0.000465           0      4730           recvfrom
  0.29    0.000294           0      2365           socket
  0.25    0.000256           0      9078           gettimeofday
  0.25    0.000251           0      7095           epoll_ctl
  0.06    0.000061           0      2365           ioctl
  0.04    0.000045           0      2365           getsockopt
------ ----------- ----------- --------- --------- ----------------
100.00    0.102169                 59412      5782 total
```

将nginx打印日志配置buffer缓冲，并分别设置256k,512k,1024k进行压测调优，发现buffer配置为512k时，write耗时性能最优(耗时比占用0.5%)，所以修改nginx配置如下:

```
#nginx配置
#使用http长连
upstream backend {
        server 127.0.0.1:8878;
        keepalive 16; #每个worker维持16个http长连
}

#增大nginx的log日志buffer为512k
server {
    ...
    access_log /data/log/nginx/access_proxy-80.log main buffer=512k;
}
```

修改上线之后，监控尚未发现耗时超过3s的日志，问题已经解决。

#### 4.新的问题

nginx配置修改上线1天之后，发现nginx极少的error日志:

```
2020/03/16 02:05:02 [error] 21533#0: *8125632065 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 10.209.156.213, server: api.iot.360.cn, request: "POST /user/getDeviceUserList HTTP/1.1", upstream: "http://127.0.0.1:8878/user/getDeviceUserList", host: "10.209.221.234"
```

> *果然是解决了一个问题的同时，又出现了另一个问题~*

对于`connection reset by peer`问题，一顿百度google之后，没有发现令人满意的解决方案。又重新开始思考http1.1协议及keep alive，并发现了之前自己错误的观点:

```
#[错误]如下nginx观点错误
一直以为nginx(1.14.2)里的upstream长连使用的超时时间是keepalive_timeout,
比如设定keepalive_timeout=25s，则表示一个upstream长连最多维持25s(生命周期)，
包括基于这条连接的所有请求，处理，响应，空闲时间。

#[错误]如下golang的net.http包 httpServer观点错误
server := http.Server{
		...
		IdleTimeout: time.Second * 28, //空闲连接超时时间
	}
一直以为idleTimeout指的是一个http长连维护最多28s(生命周期), 包括这期间的所有请求，处理，响应，空闲时间。
```

其实http1.1的keepalive指的是连接空闲时的超时时间,不包括请求，处理，响应时间，如果一个http长连接一直有流量，并且两次流量的间隔时间不超过keepalive设定，则这条连接会一直存活。**nginx的upstream其实并没有超时时间，一旦建立，不会主动关闭，只有服务端主动发送FIN包，nginx才会把ack这个连接关闭。**

那为什么nginx会出现connect reset by peer呢?使用tcpdump抓包之后，原因就水落石出:

```
#使用tcpdump抓包
#80为nginx对外暴露http服务的端口
#8878为api-gateway的http监听端口
> tcpdump -i any port 8878 -nn
...
#nginx向api-gateway发送http请求(正常)
02:04:34.085819 IP localhost.56316 > localhost.8878: Flags [P.], seq 799687:800138, ack 492088, win 1024, options [nop,nop,TS val 2055170395 ecr 2055156376], length
 451
#api-gateway响应窗口,响应http数据(正常)
02:04:34.087436 IP localhost.8878 > localhost.56316: Flags [P.], seq 492088:492419, ack 800138, win 1024, options [nop,nop,TS val 2055170396 ecr 2055170395], length
 331
#nginx响应窗口(正常)
02:04:34.087446 IP localhost.56316 > localhost.8878: Flags [.], ack 492419, win 1022, options [nop,nop,TS val 2055170396 ecr 2055170396], length 0
#间隔了28s15ms, nginx向api-gateway发送http请求
02:05:02.102019 IP localhost.56316 > localhost.8878: Flags [P.], seq 800138:800589, ack 492419, win 1024, options [nop,nop,TS val 2055198411 ecr 2055170396], length
 451
#因为api-gateway配置的http超时时间是28s,此时apigateway准备，但尚未向nginx发送FIN包
#但是nginx也巧合的在刚刚超时的时间点先PUSH了一个包
#所以api-gateway响应reset包
02:05:02.102980 IP localhost.8878 > localhost.56316: Flags [R.], seq 492419, ack 800589, win 1024, options [nop,nop,TS val 2055198412 ecr 2055198411], length 0
 ...
```

由于在api-gateway准备对连接进行超时关闭时(idleTimeout为28s)，nginx却巧合在此时仍然通过该连接发送请求，所以收到了R包，报了`reset by peer`.在工程实践上，这种问题只有靠双方不停的心跳去解决。但是nginx(1.14.2版本)，尚无对upstream长连接进行超时的配置，所以暂无较好的解决方案。如下为两种尝试方案:

```
1.api-gateway的超时设置为最大，或者永不超时
2.将nginx的upstream keepalive去掉，使用http短连
```

鉴于此种情况出现率极低，在业务层面可以容忍，所以暂时忽略。

#### 5.问题结论

目前线上服务器8core16g，在单机qps 800左右时，偶现了nginx请求api-gateway超时情况，对nginx最终做了如下优化:

```
#1.增加了nginx的backlog
listen 80 backlog=65535;
#2.修改upstream为长连接
upstream backend {
        server 127.0.0.1:8878;
        keepalive 16; #每个worker维持16个http长连
}
#3.增加日志buffer
server {
    ...
    access_log /data/log/nginx/access_proxy-80.log main buffer=512k;
}
```

但同时导致了`connection reset by peer`问题，并分析了其原因。

*果然是填了一个坑，又挖了一个新坑，把这个坑留给以后的同胞们去填！*

